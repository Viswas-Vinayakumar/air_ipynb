{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zdjMtTR3GcoK"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load your dataset\n",
        "from supabase import create_client\n",
        "import pandas as pd\n",
        "\n",
        "SUPABASE_URL = \"https://qmjpxafyxikhynczflch.supabase.co\"\n",
        "SUPABASE_KEY = \"sb_publishable_3yT3_nQMXBfLu27TyCU9UQ_jI57jX4q\"\n",
        "\n",
        "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# Ensure the 'date' column is in datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Get the last date in the dataset\n",
        "last_date = df['date'].max()\n",
        "\n",
        "# Calculate the date 30 days before the last date\n",
        "start_date = last_date - pd.Timedelta(days=30)\n",
        "\n",
        "# Filter the dataframe for the last 30 days, excluding today\n",
        "last_30_days_df = df[(df['date'] > start_date) & (df['date'] < last_date)]\n",
        "\n",
        "# Save the filtered data to a new CSV\n",
        "last_30_days_df.to_csv('last_30_days_data.csv', index=False)\n",
        "\n",
        "print(\"Filtered data saved to 'last_30_days_data.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgslp6OT-w8w"
      },
      "outputs": [],
      "source": [
        "response = supabase.table(\"air_quality_raw\") \\\n",
        "    .select(\"date, pm25\") \\\n",
        "    .order(\"date\", desc=False) \\\n",
        "    .execute()\n",
        "\n",
        "df = pd.DataFrame(response.data)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjuuztZ_Inbg"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=[' co', ' so2'], inplace=True)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYhmokIbLLfB"
      },
      "outputs": [],
      "source": [
        "df.columns = df.columns.str.strip()\n",
        "print(df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NJXN50yNMb_"
      },
      "outputs": [],
      "source": [
        "cols = ['pm25', 'pm10', 'no2', 'o3']\n",
        "df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gy-Ulv9KNwNV"
      },
      "outputs": [],
      "source": [
        "# Convert Date and Set Index\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.sort_values(by='date').reset_index(drop=True)\n",
        "\n",
        "# interprolate values (linear)\n",
        "df[['pm25', 'pm10', 'no2', 'o3']] = df[['pm25', 'pm10', 'no2', 'o3']].interpolate(method='linear')\n",
        "\n",
        "# Fill remaining NaN at start/end using forword fill then backword fill\n",
        "df[['pm25', 'pm10', 'no2', 'o3']] =df[['pm25', 'pm10', 'no2', 'o3']].ffill().bfill()\n",
        "\n",
        "df.set_index('date', inplace=True)\n",
        "print(df.info())\n",
        "print(df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAXRSDseOREJ"
      },
      "outputs": [],
      "source": [
        "missing_cols = df.columns[df.isnull().any()]\n",
        "print(\"Columns with Missing Values:\")\n",
        "print(missing_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIpLMNCGO-VN"
      },
      "outputs": [],
      "source": [
        "print(df[df.isna().any(axis=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSdPG7gbPt5c"
      },
      "outputs": [],
      "source": [
        "# Replace '-' with NaN and forward fill\n",
        "df.replace('-', np.nan, inplace=True)\n",
        "df.ffill(inplace=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx-onbR9QdEM"
      },
      "outputs": [],
      "source": [
        "print(df.head(30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2k_l5EwQ4wG"
      },
      "outputs": [],
      "source": [
        "# Select Features (Pollutant Columns)\n",
        "features = ['pm25', 'pm10', 'no2', 'o3']\n",
        "data = df[features]\n",
        "\n",
        "# Handle Missing Values (Using Simple Interpolation)\n",
        "# This fills gaps using a linear trend between known values.\n",
        "data = data.interpolate(method='linear')\n",
        "\n",
        "# Verify no more missing data\n",
        "print(\"\\nMissing values after interpolation:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\nProcessed Data Head:\")\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZjxOEs0RGXu"
      },
      "outputs": [],
      "source": [
        "# Scaling and Sequence Generation\n",
        "\n",
        "# Scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Define Sequence Parameters\n",
        "N_IN = 7      # Lookback window (7 days)\n",
        "N_OUT = 2     # Prediction horizon (2 days)\n",
        "N_FEATURES = len(features) # 4 pollutants\n",
        "\n",
        "# Function to Create Sequences (Sliding Window)\n",
        "def create_sequences(data, n_in, n_out):\n",
        "    X, y = [], []\n",
        "    # Loop from the starting point up to the end minus the total length of the sequence\n",
        "    for i in range(len(data) - n_in - n_out + 1):\n",
        "        # Input sequence (7 days)\n",
        "        end_ix = i + n_in\n",
        "        X.append(data[i:end_ix, :])\n",
        "\n",
        "        # Output sequence (2 days starting right after the input ends)\n",
        "        out_ix = end_ix + n_out\n",
        "        y.append(data[end_ix:out_ix, :])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the sequences\n",
        "X, y = create_sequences(scaled_data, N_IN, N_OUT)\n",
        "\n",
        "print(f\"\\nShape of Input (X): (Samples, Lookback Days, Features) -> {X.shape}\")\n",
        "print(f\"Shape of Output (y): (Samples, Prediction Horizon, Features) -> {y.shape}\")\n",
        "\n",
        "# Chronological Train/Test Split (80% Train, 20% Test)\n",
        "# Time series data must be split chronologically.\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"Train/Test Split: {len(X_train)} training samples, {len(X_test)} test samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2Sx3PWvRe4r"
      },
      "outputs": [],
      "source": [
        "# Build, Train, and Evaluate the LSTM Model\n",
        "\n",
        "#  Build the Model\n",
        "model = Sequential()\n",
        "\n",
        "# LSTM Layer: 50 units, returns sequences for the next layer\n",
        "model.add(LSTM(50, activation='relu', input_shape=(N_IN, N_FEATURES)))\n",
        "\n",
        "# Output Layer: Dense layer to predict the 4 features (pollutants) for the 2 days (N_OUT)\n",
        "# flatten the output (N_OUT * N_FEATURES = 8 values)\n",
        "model.add(Dense(N_OUT * N_FEATURES))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Display Model Summary\n",
        "print(\"\\nModel Summary:\")\n",
        "model.summary()\n",
        "\n",
        "# Training the Model\n",
        "# EarlyStopping prevents overfitting by stopping training if validation loss doesn't improve.\n",
        "callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train.reshape(X_train.shape[0], -1), # Reshape y_train for the model\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1, # Use 10% of training data for validation\n",
        "                    callbacks=[callback])\n",
        "\n",
        "# Evaluate Model on the Test Set\n",
        "test_loss = model.evaluate(X_test, y_test.reshape(X_test.shape[0], -1), verbose=0)\n",
        "print(f\"\\nTest Set Mean Squared Error (MSE): {test_loss:.4f}\")\n",
        "\n",
        "# Make Predictions on Test Set\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "\n",
        "# Reshape predictions and actuals back to (Samples, N_OUT, N_FEATURES)\n",
        "y_pred_scaled = y_pred_scaled.reshape(y_test.shape)\n",
        "\n",
        "# Inverse Transform (Rescale to original values)\n",
        "# To inverse transform (must first flatten the 3D arrays to 2D)\n",
        "y_test_original = scaler.inverse_transform(y_test.reshape(-1, N_FEATURES))\n",
        "y_pred_original = scaler.inverse_transform(y_pred_scaled.reshape(-1, N_FEATURES))\n",
        "\n",
        "# Reshape back to 3D for comparison plots\n",
        "y_test_original = y_test_original.reshape(y_test.shape)\n",
        "y_pred_original = y_pred_original.reshape(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H30eSy67STeT"
      },
      "outputs": [],
      "source": [
        "# Final 2-Day Forecast\n",
        "\n",
        "# Prepare the Final Input\n",
        "# Take the last N_IN (7) days from the scaled dataset\n",
        "last_7_days_scaled = scaled_data[-N_IN:]\n",
        "\n",
        "# Reshape the input to match model's expected input shape: (1, N_IN, N_FEATURES)\n",
        "X_input = last_7_days_scaled.reshape(1, N_IN, N_FEATURES)\n",
        "\n",
        "# Generate the Forecast\n",
        "forecast_scaled = model.predict(X_input)\n",
        "\n",
        "# Reshape the forecast (1, N_OUT * N_FEATURES) to (N_OUT, N_FEATURES) for inverse scaling\n",
        "forecast_scaled = forecast_scaled.reshape(N_OUT, N_FEATURES)\n",
        "\n",
        "# Inverse Transform the Forecast to Original Units\n",
        "final_forecast_original = scaler.inverse_transform(forecast_scaled)\n",
        "\n",
        "# Create a Forecast DataFrame\n",
        "last_date = df.index[-1]\n",
        "forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=N_OUT, freq='D')\n",
        "\n",
        "forecast_df = pd.DataFrame(final_forecast_original,\n",
        "                           index=forecast_dates,\n",
        "                           columns=features)\n",
        "\n",
        "print(\"\\n\\n#####################################################\")\n",
        "print(\"## ✅ 2-DAY AIR QUALITY FORECAST (NEXT 48 HOURS) ✅ ##\")\n",
        "print(\"#####################################################\")\n",
        "print(forecast_df)\n",
        "print(\"-----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KEN0wL74RTI"
      },
      "outputs": [],
      "source": [
        "# AQI CALCULATION (POST-PROCESSING, NO MODEL CHANGE) ---\n",
        "\n",
        "# AQI breakpoint tables (CPCB – India)\n",
        "aqi_breakpoints = {\n",
        "    \"pm25\": [(0,30,0,50),(31,60,51,100),(61,90,101,200),(91,120,201,300),(121,250,301,400),(251,500,401,500)],\n",
        "    \"pm10\": [(0,50,0,50),(51,100,51,100),(101,250,101,200),(251,350,201,300),(351,430,301,400),(431,600,401,500)],\n",
        "    \"no2\":  [(0,40,0,50),(41,80,51,100),(81,180,101,200),(181,280,201,300),(281,400,301,400),(401,1000,401,500)],\n",
        "    \"o3\":   [(0,50,0,50),(51,100,51,100),(101,168,101,200),(169,208,201,300),(209,748,301,400)]\n",
        "}\n",
        "\n",
        "def calculate_sub_aqi(pollutant, value):\n",
        "    for bp_lo, bp_hi, i_lo, i_hi in aqi_breakpoints[pollutant]:\n",
        "        if bp_lo <= value <= bp_hi:\n",
        "            return ((i_hi - i_lo)/(bp_hi - bp_lo)) * (value - bp_lo) + i_lo\n",
        "    return None\n",
        "\n",
        "# Calculate AQI for each predicted day\n",
        "aqi_values = []\n",
        "aqi_categories = []\n",
        "\n",
        "for _, row in forecast_df.iterrows():\n",
        "    sub_indices = [\n",
        "        calculate_sub_aqi('pm25', row['pm25']),\n",
        "        calculate_sub_aqi('pm10', row['pm10']),\n",
        "        calculate_sub_aqi('no2', row['no2']),\n",
        "        calculate_sub_aqi('o3', row['o3'])\n",
        "    ]\n",
        "    final_aqi = int(max(sub_indices))\n",
        "    aqi_values.append(final_aqi)\n",
        "\n",
        "    if final_aqi <= 50:\n",
        "        aqi_categories.append(\"Good\")\n",
        "    elif final_aqi <= 100:\n",
        "        aqi_categories.append(\"Satisfactory\")\n",
        "    elif final_aqi <= 200:\n",
        "        aqi_categories.append(\"Moderate\")\n",
        "    elif final_aqi <= 300:\n",
        "        aqi_categories.append(\"Poor\")\n",
        "    elif final_aqi <= 400:\n",
        "        aqi_categories.append(\"Very Poor\")\n",
        "    else:\n",
        "        aqi_categories.append(\"Severe\")\n",
        "\n",
        "# Add AQI to forecast table\n",
        "forecast_df[\"AQI\"] = aqi_values\n",
        "forecast_df[\"AQI_Category\"] = aqi_categories\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "record = {\n",
        "    \"forecast_date\": (datetime.utcnow() + timedelta(days=1)).isoformat(),\n",
        "    \"day_ahead\": 1,\n",
        "    \"pm25\": float(pred_pm25),\n",
        "    \"aqi\": int(aqi),\n",
        "    \"aqi_level\": aqi_category(aqi),\n",
        "    \"model_name\": \"Teammate_Model_Google_Aligned\"\n",
        "}\n",
        "\n",
        "supabase.table(\"air_quality_forecast\").insert(record).execute()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
